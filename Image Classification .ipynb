{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2lipeNI33Pbe"
   },
   "source": [
    "## Question 1. MNIST classification using multi-class logistic regression \n",
    "\n",
    "Consider a L2-regularized multi-class logistic regression model using the MNIST dataset. \n",
    "\n",
    "The model is given by\n",
    "$\\hat{y}=\\sigma(W^TX+\\mathbf{b})$, where $\\sigma(\\cdot)$ is \n",
    "the softmax function \n",
    "$$\\sigma_j(x) = \\frac{e^{z_j}}{\\sum_{k=1}^{K}e^{z_k}}$$\n",
    "\n",
    "The objective is the cross-entropy loss function:\n",
    "\n",
    "$$l(\\hat{y},y)=-\\sum_{k=1}^{K}y_k\\log(\\hat{y_k})$$\n",
    "\n",
    "where $K$ is the number of classes and $\\hat{y_{k}}$ is the output probability that a sample belongs to class k, with a L2 regularizer on the weight parameters $W$, i.e., $\\lambda||W||_2^2$, the L2 norm of the vectorized $W$ where $\\lambda$ is a hyper-parameter. \n",
    "\n",
    "The hyper-parameter settings are given as below:\n",
    "- minibatch size = 128 \n",
    "- starting learning rate $\\eta^{(0)}=0.001$\n",
    "- decaying learning rate $\\eta^{(t)}=\\eta^{(0)}/\\sqrt{t}$ during training where $t$ is the number of epochs \n",
    "- Momentum = 0.7\n",
    "- $\\lambda=0.01$\n",
    "- total number of epoches = 45\n",
    "\n",
    "**Task:** Evaluate and plot **the average loss per epoch** versus the number of epochs for the training dataset, for the following optimization algorithms:\n",
    "- Mini-batch gradient descent\n",
    "- Mini-batch AdaGrad\n",
    "- Mini-batch gradient descent with Nesterov’s momentum\n",
    "- Mini-batch Adam \n",
    "\n",
    "Discuss how the performance of different optimization algorithms compare to each other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qFgWvvaa3Pbj"
   },
   "outputs": [],
   "source": [
    "# Load required libraries\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.datasets import mnist, cifar10\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib.pyplot import figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bHmFkIdy3Pbj",
    "outputId": "b2577d75-bde1-4ed1-a368-9c8d9d7a864c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n",
      "(10000, 28, 28) (10000,)\n",
      "[5 0 4 1]\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "255 0\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "# loading the mnist dataset \n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# print shape of training data\n",
    "print(x_train.shape, y_train.shape)\n",
    "\n",
    "# print shape of testing data\n",
    "print(x_test.shape, y_test.shape)\n",
    "\n",
    "# print out first 4 examples\n",
    "print(y_train[:4]) \n",
    "\n",
    "# see unique labels in training data\n",
    "print(np.unique(y_train))\n",
    "\n",
    "# Range of values in training data\n",
    "print(np.max(x_train), np.min(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PtJPTvN_3Pbk",
    "outputId": "f8cfe097-5800-4ab2-dc44-0eb15a8c0246"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "1.0 0.0\n"
     ]
    }
   ],
   "source": [
    "# preprocessing the training data\n",
    "x_train_p = np.reshape(x_train/255. , (-1,28*28))\n",
    "\n",
    "# shape of preprocessed training data\n",
    "print(x_train_p.shape)\n",
    "\n",
    "# Range of values in preprocessed training data\n",
    "print(np.max(x_train_p), np.min(x_train_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVF7iKq43Pbl"
   },
   "source": [
    "# 1.) Training using Mini-batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-YqVXZzI3Pbl"
   },
   "outputs": [],
   "source": [
    "# defining callback for reducing learning rate based on epoch\n",
    "def scheduler(epoch, lr):\n",
    "    \n",
    "    # set base learning rate \n",
    "    base_lr = 0.001\n",
    "    \n",
    "    # +1 because epoch starts at 0\n",
    "    lr_new  = base_lr/np.sqrt(epoch+1)\n",
    "    return lr_new \n",
    "\n",
    "# set callback to be used during model.fit()\n",
    "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gVwc7jWH3Pbl",
    "outputId": "e03c37ec-0a0a-4e7b-ca91-da5e62457bc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "# defining the logistic regression model \n",
    "model = Sequential([Dense(units=10, activation=\"softmax\", kernel_regularizer=tf.keras.regularizers.l2(0.01), input_shape=(28*28,))]) \n",
    "\n",
    "# Check out summary\n",
    "model.summary()\n",
    "\n",
    "# Set number of epochs\n",
    "num_epochs = 45\n",
    "\n",
    "# Set batch size #\n",
    "num_batch_size = 128\n",
    "\n",
    "# compile model\n",
    "model.compile(optimizer = tf.keras.optimizers.SGD(learning_rate=0.001), loss=\"sparse_categorical_crossentropy\")\n",
    "\n",
    "# History for plot later on\n",
    "History_gd = model.fit(x_train_p,y_train, epochs = num_epochs, batch_size = 128,\n",
    "                       # can change verbose=1 to see the logs\n",
    "                       verbose=0, callbacks=[callback]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jp9fHNVU3Pbl"
   },
   "source": [
    "# 2.) Training using Mini-batch AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "HA3ZG3xX3Pbm"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(1)\n",
    "model = Sequential([ Dense(units=10, activation=\"softmax\", kernel_regularizer=tf.keras.regularizers.l2(0.01), input_shape=(28*28,))]) \n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.001), loss=\"sparse_categorical_crossentropy\")\n",
    "\n",
    "# without learning rate decay  \n",
    "# History_ada_grad = model.fit(x_train_p,y_train, epochs=num_epochs, batch_size = num_batch_size, verbose=0)\n",
    "\n",
    "# with learning rate decay\n",
    "History_ada_grad = model.fit(x_train_p,y_train, epochs = num_epochs, batch_size = num_batch_size, verbose = 0, callbacks=[callback]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTZ7tDG93Pbm"
   },
   "source": [
    "# 3.) Training using Mini-batch gradient descent with Nesterov’s momentum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "hnuJVn6F3Pbm"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(1)\n",
    "model = Sequential([ Dense(units=10, activation=\"softmax\", kernel_regularizer=tf.keras.regularizers.l2(0.01), input_shape=(28*28,))]) \n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.7 , nesterov=True), loss=\"sparse_categorical_crossentropy\")\n",
    "\n",
    "# without learning rate decay  \n",
    "#H_nestrov = model.fit(x_train_p,y_train, epochs=3, batch_size=128, verbose=1)\n",
    "\n",
    "# with learning rate decay\n",
    "History_nestrov = model.fit(x_train_p,y_train, epochs=num_epochs, batch_size = num_batch_size, callbacks=[callback],verbose=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8klKOw5p3Pbm"
   },
   "source": [
    "# 4.) Training using Mini-batch Adam\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "S6aa9_la3Pbn"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(1)\n",
    "model = Sequential([ Dense(units=10, activation=\"softmax\", kernel_regularizer=tf.keras.regularizers.l2(0.01), input_shape=(28*28,))]) \n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=\"sparse_categorical_crossentropy\")\n",
    "\n",
    "# without learning rate decay  \n",
    "#History_adam = model.fit(x_train_p,y_train, epochs=num_epochs, batch_size=128, verbose=0) \n",
    "\n",
    "# with learning rate decay\n",
    "History_adam = model.fit(x_train_p,y_train, epochs=num_epochs, batch_size = num_batch_size, callbacks=[callback],verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "hcRMc4kI3Pbn",
    "outputId": "21c20d51-0e5d-4958-d954-6c56b574af15"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "# plotting the average loss per epoch for the training dataset for different optimization algorithms \n",
    "num_epochs=45\n",
    "plt.figure()\n",
    "epochs = np.arange(0,num_epochs,1)\n",
    "\n",
    "loss_gd = History_gd.history[\"loss\"]\n",
    "plt.plot(epochs, loss_gd, label=\"mini-batch GD\")\n",
    "\n",
    "loss_ada_grad = History_ada_grad.history[\"loss\"]\n",
    "plt.plot(epochs, loss_ada_grad, label=\"mini-batch AdaGrad\")\n",
    "\n",
    "loss_nestrov = History_nestrov.history[\"loss\"]\n",
    "plt.plot(epochs, loss_nestrov, label=\"mini-batch GD with Nestrov\")\n",
    "\n",
    "loss_adam = History_adam.history[\"loss\"]\n",
    "plt.plot(epochs, loss_adam, label=\"mini-batch Adam\")\n",
    "\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Training loss\")\n",
    "plt.title(\"Training loss versus epoch for different optimization algorithms\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fkno868r3Pbn"
   },
   "source": [
    "### Discussion on the performance of different optimization algorithms compared to one another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AP3hHT02GMC_"
   },
   "source": [
    "When comparing optimization algorithms we want an algorithm which yields the lowest Training Loss. Amgonst the four different optimization algorithms we implemented, Mini-batch Adam had the best overall performance. One thing to note about Mini-batch Adam is that it appears to plateau rather quickly. Mini-batch gradient descent with Nesterov’s momentum performed well too. As epochs increased it narrow the gap between itself and Mini-batch Adam.  Mini-batch AdaGrad and Mini-batch gradient descent had similar results. As epochs increased their results became nearly identical. Our epochs ranged from 0 to 45, but I suspect if the range of epochs was increased we would see gradient descent, AdaGrand and gradient descent with Nesterov’s momentum improve while Adam remained constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hYMMg5AN3Pbo"
   },
   "source": [
    "## Question 2. CIFAR10 CNN: convergence of minibatch gradient descent\n",
    "\n",
    "Implement a CNN architecture that consists of 3 convolutional layers followed by a hidden fully connected layer of 1000 units. \n",
    "\n",
    "Each convolutional layer consists of a sublayer of 5x5 convolutional filters with stride 1 followed by a sublayer of 2x2 max-pool units with stride 2. Each neuron applies ReLU activation function.\n",
    "\n",
    "**Task:** Evaluate **both the training and test loss function values** versus the number of epochs. In addition, show the results by adding dropout. Comment the results. \n",
    "\n",
    "**Hints:**\n",
    "\n",
    "- Load CIFAR10 data by the following code:\n",
    "```\n",
    "from keras.datasets import cifar10\n",
    "(data_train, label_train), (data_test, label_test) = cifar10.load_data()\n",
    "```\n",
    "- In order to reduce the training time, use only the first 50 mini-batches for each epoch. \n",
    "- More specifically, at the beginning of each epoch, randomly shuffle the whole dataset training dataset. Then, only iterate through the first 50 mini-batches for one epoch training.  \n",
    "- Training on Google Colab GPU is highly recommended. The training time on 1 GPU is roughly 1 minute per epoch.  \n",
    "\n",
    "The hyper-parameter settings:\n",
    "- minibatch size = 128 \n",
    "- learning rate = 0.001\n",
    "- total number of epoches = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xOeul1_o3Pbo"
   },
   "source": [
    "### Loading and preprocessing the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9zqaUqDE3Pbo",
    "outputId": "0ac91a73-bf09-47b0-96ce-f51713731cba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (50000, 1)\n",
      "(10000, 32, 32, 3) (10000, 1)\n",
      "[[6]\n",
      " [9]\n",
      " [9]\n",
      " [4]]\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "255 0\n",
      "(50000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n",
      "1.0 0.0\n"
     ]
    }
   ],
   "source": [
    "# loading the cifar10 dataset \n",
    "(data_train, label_train), (data_test, label_test) = cifar10.load_data()\n",
    "\n",
    "# Insights into cifar data\n",
    "print(data_train.shape, label_train.shape)\n",
    "print(data_test.shape, label_test.shape)\n",
    "print(label_train[:4])\n",
    "print(np.unique(label_train))\n",
    "\n",
    "# See range of values in training data\n",
    "print(np.max(data_train), np.min(data_train))\n",
    "\n",
    "# preprocessing the training dataset \n",
    "data_train_p = data_train/255. \n",
    "print(data_train_p.shape)\n",
    "\n",
    "# preprocessing the testing dataset\n",
    "data_test_p = data_test/255.\n",
    "print(data_test_p.shape)\n",
    "\n",
    "# See range of values in preprocessed training data (notice diff in max)\n",
    "print(np.max(data_train_p), np.min(data_train_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U0zZBGwG3Pbp",
    "outputId": "fe2503f2-3f60-41a6-ab83-448deb1d65a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 64)        4864      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 32)        51232     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 4, 4, 10)          8010      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 2, 2, 10)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1000)              41000     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                10010     \n",
      "=================================================================\n",
      "Total params: 115,116\n",
      "Trainable params: 115,116\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# defining the model \n",
    "model = Sequential()\n",
    "\n",
    "# convolutional layer w/ sublayer of 5x5 convolutional filters & stride 1\n",
    "model.add(Conv2D(filters=64, kernel_size=(5,5), strides=(1,1), padding=\"same\", activation=\"relu\", input_shape=(32,32,3)))\n",
    "\n",
    "# sublayer of 2x2 max-pool units with stride 2\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding='same'))\n",
    "\n",
    "# DROPOUT LAYER\n",
    "model.add(Dropout(rate=0.2)) \n",
    "\n",
    "# convolutional layer w/ sublayer of 5x5 convolutional filters & stride 1\n",
    "model.add(Conv2D(filters=32, kernel_size=(5,5), strides=(1,1), padding = \"same\", activation=\"relu\"))\n",
    "\n",
    "# sublayer of 2x2 max-pool units with stride 2\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding='same'))\n",
    "\n",
    "# convolutional layer w/ sublayer of 5x5 convolutional filters & stride 1\n",
    "model.add(Conv2D(filters=10, kernel_size=(5,5), strides=(1,1), padding=\"valid\", activation=\"relu\"))\n",
    "\n",
    "# sublayer of 2x2 max-pool units with stride 2\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding='valid'))\n",
    "\n",
    "# Flatten\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add Dense layer\n",
    "model.add(Dense(1000 , activation=\"relu\"))\n",
    "\n",
    "# DROPOUT LAYER\n",
    "model.add(Dropout(rate=0.5))\n",
    "\n",
    "# Add final Dense layer\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "# see summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HqiwQAJG3Pbq",
    "outputId": "c35b3ec1-d7fd-41c1-ac41-8714c2e136d6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 110s 2ms/sample - loss: 1.7041 - val_loss: 1.5058\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 117s 2ms/sample - loss: 1.3663 - val_loss: 1.2686\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 108s 2ms/sample - loss: 1.2367 - val_loss: 1.1792\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 117s 2ms/sample - loss: 1.1510 - val_loss: 1.1281\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 117s 2ms/sample - loss: 1.0893 - val_loss: 1.0604\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 135s 3ms/sample - loss: 1.0336 - val_loss: 1.0395\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 139s 3ms/sample - loss: 0.9959 - val_loss: 1.0487\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 121s 2ms/sample - loss: 0.9583 - val_loss: 0.9465\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 117s 2ms/sample - loss: 0.9241 - val_loss: 0.9512\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 126s 3ms/sample - loss: 0.9007 - val_loss: 0.9292\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 136s 3ms/sample - loss: 0.8775 - val_loss: 0.9155\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 135s 3ms/sample - loss: 0.8619 - val_loss: 0.9012\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 124s 2ms/sample - loss: 0.8486 - val_loss: 0.9662\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 139s 3ms/sample - loss: 0.8180 - val_loss: 0.8903\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 129s 3ms/sample - loss: 0.8124 - val_loss: 0.8547\n",
      "Epoch 16/50\n",
      "43392/50000 [=========================>....] - ETA: 16s - loss: 0.7893"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-78ef4825d42e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# fit with epoches = 50, includes validation data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mHistory2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_train_p\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata_test_p\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_test\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\Robbie\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\.conda\\envs\\Robbie\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Robbie\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Robbie\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Robbie\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Robbie\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    597\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Robbie\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Robbie\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Robbie\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Robbie\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\.conda\\envs\\Robbie\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# compiling the model with adam optmizer\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\") # 0.001 is the default learning rate for Adam\n",
    "\n",
    "# fit with epoches = 50, includes validation data\n",
    "num_epochs = 50\n",
    "History2 = model.fit(data_train_p, label_train, validation_data= (data_test_p, label_test) , batch_size= 128, epochs=num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "Q2pDCTOc3Pbq",
    "outputId": "ec086a9c-cb0a-4543-b955-ee90537462d7"
   },
   "outputs": [],
   "source": [
    "# plotting training and testing loss \n",
    "epochs = np.arange(0,num_epochs,1)\n",
    "train_loss = History2.history[\"loss\"]\n",
    "# also called validation loss (Although test and validation loss are diff in ML)\n",
    "test_loss  = History2.history[\"val_loss\"] \n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_loss, label=\"train loss\")\n",
    "plt.plot(epochs, test_loss, label=\"test loss\" )\n",
    "plt.title(\"Training and Test loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FaOl5hPu3Pbq",
    "outputId": "a223399e-92b2-46a8-aa0a-7e5d19a6fb06"
   },
   "outputs": [],
   "source": [
    "# model training using only the 50 first mini-batches after randomly shuffling data on each epoch \n",
    "num_epochs = 100\n",
    "mini_batch_size  = 128\n",
    "num_mini_batches = 50\n",
    "num_full_train_samples = data_train_p.shape[0]\n",
    "num_train_samples_per_epoch = mini_batch_size*num_mini_batches\n",
    "\n",
    "train_losses = []\n",
    "test_losses  = []\n",
    "for i in range(num_epochs):\n",
    "    indices = np.random.choice( num_full_train_samples , size=(num_train_samples_per_epoch,) , replace=False)\n",
    "    data_train_set = data_train_p[indices]\n",
    "    label_train_set = label_train[indices]\n",
    "    H = model.fit(data_train_set, label_train_set, validation_data= (data_test_p, label_test) , batch_size= 128, epochs=1)\n",
    "    train_losses = train_losses + H.history[\"loss\"]\n",
    "    test_losses  = test_losses  + H.history[\"val_loss\"]\n",
    "\n",
    "\n",
    "print(len(train_losses), len(test_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "obQfbxC73Pbq",
    "outputId": "cfda6cb7-a3ca-4884-ab90-08b66f53429e"
   },
   "outputs": [],
   "source": [
    "# plotting training and testing loss \n",
    "epochs = np.arange(0,num_epochs,1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_losses, label=\"train loss\")\n",
    "plt.plot(epochs, test_losses, label=\"test loss\" )\n",
    "plt.title(\"Training and Test loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xp-T_Omf3Pbn"
   },
   "source": [
    "The visualization above illustrates the Training and Testing loss over 100 epochs. As you can see they follow a similar path. The training loss decreases at a faster rate. Towards the end of the X-axis the gap between training loss and test increass increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4sUqu5N3Pbr"
   },
   "source": [
    "## Question 3. CIFAR10 image classification\n",
    "\n",
    "Design and implement a convolutional neural network for the CIFAR10 image classification task aiming to achieve a high test accuracy. Evaluate the classification accuracy by reporting top-1 and top-5 test error rates. \n",
    "\n",
    "**Task:** Plot the loss function, top-1 error rate and top-5 error rate per epoch versus the number of epochs for the training and the test dataset. \n",
    "Make sure to well describe and justify your network architecture design choices. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wB0oIZ-M3Pbr"
   },
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AkI6-D7B3Pbr",
    "outputId": "91333661-de7a-4820-c4d6-a3828754be27"
   },
   "outputs": [],
   "source": [
    "# defining the model \n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(5,5), strides=(1,1), padding=\"same\", activation=\"relu\", input_shape=(32,32,3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding='valid'))\n",
    "model.add(Conv2D(filters=10, kernel_size=(5,5), strides=(1,1), padding=\"valid\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding='valid'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1000 , activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "# see summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JeI-9uf2iHVb"
   },
   "source": [
    "Our base model is a simple CNN with two CNN layers. We wanted to start off by having a base model to compare our more complex models too. With the two CNN layers we also add two MaxPooling layers before flatting the network. As you can see the base model's architecture lead to a small number of overall paramters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F7HQ-hkn3Pbr",
    "outputId": "331acc6e-ddcb-4724-cb72-fb89c35dc663"
   },
   "outputs": [],
   "source": [
    "# compiling the model with adam optmizer and specifying the accuracy metrics (top-1 and top-5 accuracy)\n",
    "acc_metrics = [tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1, name='Top_1_Acc') , \n",
    "               tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5, name='Top_5_Acc')]\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics= acc_metrics) \n",
    "\n",
    "\n",
    "# training the model \n",
    "num_epochs = 100\n",
    "mini_batch_size  = 128\n",
    "num_mini_batches = 50\n",
    "num_full_train_samples = data_train_p.shape[0]\n",
    "num_train_samples_per_epoch = mini_batch_size*num_mini_batches\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_top_1_acc = []\n",
    "train_top_5_acc = []\n",
    "test_top_1_acc = []\n",
    "test_top_5_acc = []\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    indices = np.random.choice(num_full_train_samples , size=(num_train_samples_per_epoch,) , replace=False)\n",
    "    data_train_set = data_train_p[indices]\n",
    "    label_train_set = label_train[indices]\n",
    "    H = model.fit(data_train_set, label_train_set, validation_data= (data_test_p, label_test) , batch_size= mini_batch_size, epochs = 1)\n",
    "    train_losses.append(H.history[\"loss\"])\n",
    "    test_losses.append(H.history[\"val_loss\"])\n",
    "    train_top_1_acc.append(H.history[\"Top_1_Acc\"])\n",
    "    train_top_5_acc.append(H.history[\"Top_5_Acc\"])\n",
    "    test_top_1_acc.append(H.history[\"val_Top_1_Acc\"])\n",
    "    test_top_5_acc.append(H.history[\"val_Top_5_Acc\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 574
    },
    "id": "W1wQX4j-3Pbr",
    "outputId": "c8922295-12ae-4f51-e8da-0762a92b6dbd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plotting the results \n",
    "train_top_1_error = (1 - np.array(train_top_1_acc))*100\n",
    "test_top_1_error  = (1 - np.array(test_top_1_acc))*100\n",
    "train_top_5_error = (1 - np.array(train_top_5_acc))*100\n",
    "test_top_5_error  = (1 - np.array(test_top_5_acc))*100\n",
    "\n",
    "epochs = np.arange(0,num_epochs,1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_losses, label=\"train loss\")\n",
    "plt.plot(epochs, test_losses, label=\"test loss\" )\n",
    "plt.title(\"Training and Test loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_top_1_error, label=\"top-1 train error rate\")\n",
    "plt.plot(epochs, test_top_1_error, label=\"top-1 test error rate\")\n",
    "plt.plot(epochs, train_top_5_error, label=\"top-5 train error rate\")\n",
    "plt.plot(epochs, test_top_5_error, label=\"top-5 test error rate\")\n",
    "plt.title(\"Top-1 and top-5 train and test set error rates\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test loss gradual increases at 40 epochs plus, while the training loss always decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSMvCCAs3Pbs"
   },
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qbwdg8mt3Pbs",
    "outputId": "8028389f-1041-4211-c3e5-49ef17c5678e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# defining the model \n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(5,5), strides=(1,1), padding=\"same\", activation=\"relu\", input_shape=(32,32,3)))\n",
    "\n",
    "# Adding Batch Normalization\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding='valid'))\n",
    "model.add(Conv2D(filters=32, kernel_size=(5,5), strides=(1,1), padding=\"valid\", activation=\"relu\"))\n",
    "\n",
    "# Adding drop out\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Adding Batch Normalization\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding='valid'))\n",
    "\n",
    "# Adding Conv2D, Batch Normalization\n",
    "model.add(Conv2D(filters=64, kernel_size=(5,5), strides=(1,1), padding=\"same\", activation=\"relu\", input_shape=(32,32,3)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Adding drop out\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1000 , activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "# see summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the model more complex by introducing more CNN layers. We add dropout layers to decrease the chance of overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PkG-hAJG3Pbs",
    "outputId": "d512ff5e-c0b8-43c7-f340-70693a423122"
   },
   "outputs": [],
   "source": [
    "# compiling the model with adam optmizer and specifying the accuracy metrics (top-1 and top-5 accuracy)\n",
    "acc_metrics = [tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1, name='Top_1_Acc') , \n",
    "               tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5, name='Top_5_Acc') ]\n",
    "\n",
    "# Changing learning rate by a factor of 10\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer = tf.keras.optimizers.Adam(learning_rate=0.01), metrics= acc_metrics) \n",
    "\n",
    "\n",
    "# training the model \n",
    "num_epochs = 100\n",
    "mini_batch_size  = 128\n",
    "num_mini_batches = 75\n",
    "num_full_train_samples = data_train_p.shape[0]\n",
    "num_train_samples_per_epoch = mini_batch_size*num_mini_batches\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_top_1_acc = []\n",
    "train_top_5_acc = []\n",
    "test_top_1_acc = []\n",
    "test_top_5_acc = []\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    indices = np.random.choice(num_full_train_samples , size=(num_train_samples_per_epoch,) , replace=False)\n",
    "    data_train_set = data_train_p[indices]\n",
    "    label_train_set = label_train[indices]\n",
    "    H = model.fit(data_train_set, label_train_set, validation_data= (data_test_p, label_test) , batch_size = mini_batch_size, epochs=1)\n",
    "    train_losses.append(H.history[\"loss\"])\n",
    "    test_losses.append(H.history[\"val_loss\"])\n",
    "    train_top_1_acc.append(H.history[\"Top_1_Acc\"])\n",
    "    train_top_5_acc.append(H.history[\"Top_5_Acc\"])\n",
    "    test_top_1_acc.append(H.history[\"val_Top_1_Acc\"])\n",
    "    test_top_5_acc.append(H.history[\"val_Top_5_Acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 574
    },
    "id": "i8M5OoFGFyhz",
    "outputId": "b196cfff-d0e9-404b-8d2f-d95ff59f322f"
   },
   "outputs": [],
   "source": [
    "# plotting the results \n",
    "train_top_1_error = (1 - np.array(train_top_1_acc))*100\n",
    "test_top_1_error  = (1 - np.array(test_top_1_acc))*100\n",
    "train_top_5_error = (1 - np.array(train_top_5_acc))*100\n",
    "test_top_5_error  = (1 - np.array(test_top_5_acc))*100\n",
    "\n",
    "epochs = np.arange(0,num_epochs,1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_losses, label=\"train loss\")\n",
    "plt.plot(epochs, test_losses, label=\"test loss\" )\n",
    "plt.title(\"Training and Test loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_top_1_error, label=\"top-1 train error rate\")\n",
    "plt.plot(epochs, test_top_1_error, label=\"top-1 test error rate\")\n",
    "plt.plot(epochs, train_top_5_error, label=\"top-5 train error rate\")\n",
    "plt.plot(epochs, test_top_5_error, label=\"top-5 test error rate\")\n",
    "plt.title(\"Top-1 and top-5 train and test set error rates\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJ2dghovkAqV"
   },
   "outputs": [],
   "source": [
    "In Model 2 we decided to tweak the learning rate by a factor of 10. In comparison to the base model, the test loss remains consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xz6922173Pbt"
   },
   "source": [
    "### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gZlmLLmf3Pbt",
    "outputId": "d188b45e-6a24-4a0c-e072-93fe9f524d6a"
   },
   "outputs": [],
   "source": [
    "# defining the model \n",
    "model = Sequential()\n",
    "\n",
    "# Changing kernel_size\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), strides=(1,1), padding=\"same\", activation=\"relu\", input_shape=(32,32,3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding='valid'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Changing kernel_size\n",
    "model.add(Conv2D(filters = 64, kernel_size=(3,3), strides=(1,1), padding=\"valid\", activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding='valid'))\n",
    "\n",
    "# Adding 2D Convolution Layer and Batch Normalization\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), padding=\"valid\", activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(filters = 64, kernel_size=(3,3), strides=(1,1), padding=\"valid\", activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1000 , activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "# see summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QtCMp61b3Pbt",
    "outputId": "e89b95ed-7c60-495e-b4a0-820d1b798a0b"
   },
   "outputs": [],
   "source": [
    "# compiling the model with adam optmizer and specifying the accuracy metrics (top-1 and top-5 accuracy)\n",
    "acc_metrics = [tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1, name='Top_1_Acc') , \n",
    "               tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5, name='Top_5_Acc') ]\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics= acc_metrics) \n",
    "\n",
    "\n",
    "# training the model \n",
    "num_epochs = 100\n",
    "mini_batch_size  = 128\n",
    "num_mini_batches = 75\n",
    "num_full_train_samples = data_train_p.shape[0]\n",
    "num_train_samples_per_epoch = mini_batch_size*num_mini_batches\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_top_1_acc = []\n",
    "train_top_5_acc = []\n",
    "test_top_1_acc = []\n",
    "test_top_5_acc = []\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    indices = np.random.choice(num_full_train_samples , size=(num_train_samples_per_epoch,) , replace=False)\n",
    "    data_train_set = data_train_p[indices]\n",
    "    label_train_set = label_train[indices]\n",
    "    H = model.fit(data_train_set, label_train_set, validation_data= (data_test_p, label_test) , batch_size= mini_batch_size, epochs=1)\n",
    "    train_losses.append(H.history[\"loss\"])\n",
    "    test_losses.append(H.history[\"val_loss\"])\n",
    "    train_top_1_acc.append(H.history[\"Top_1_Acc\"])\n",
    "    train_top_5_acc.append(H.history[\"Top_5_Acc\"])\n",
    "    test_top_1_acc.append(H.history[\"val_Top_1_Acc\"])\n",
    "    test_top_5_acc.append(H.history[\"val_Top_5_Acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 574
    },
    "id": "4nflncozFrCb",
    "outputId": "82b923a0-7f2d-488d-ab7c-bbef28d84219"
   },
   "outputs": [],
   "source": [
    "# plotting the results \n",
    "train_top_1_error = (1 - np.array(train_top_1_acc))*100\n",
    "test_top_1_error  = (1 - np.array(test_top_1_acc))*100\n",
    "train_top_5_error = (1 - np.array(train_top_5_acc))*100\n",
    "test_top_5_error  = (1 - np.array(test_top_5_acc))*100\n",
    "\n",
    "epochs = np.arange(0,num_epochs,1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_losses, label=\"train loss\")\n",
    "plt.plot(epochs, test_losses, label=\"test loss\" )\n",
    "plt.title(\"Training and Test loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_top_1_error, label=\"top-1 train error rate\")\n",
    "plt.plot(epochs, test_top_1_error, label=\"top-1 test error rate\")\n",
    "plt.plot(epochs, train_top_5_error, label=\"top-5 train error rate\")\n",
    "plt.plot(epochs, test_top_5_error, label=\"top-5 test error rate\")\n",
    "plt.title(\"Top-1 and top-5 train and test set error rates\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yc8s8oBK3Pbt"
   },
   "source": [
    "### Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mFCH-KNo3Pbt",
    "outputId": "e92e65a5-0b7a-4d05-b246-17df9e19c735",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# defining the model \n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(5,5), strides=(1,1), padding=\"same\", activation=\"relu\", input_shape=(32,32,3)))\n",
    "\n",
    "# Adding Batch Normalization\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding='same'))\n",
    "model.add(Conv2D(filters=10, kernel_size=(5,5), strides=(1,1), padding=\"same\", activation=\"relu\"))\n",
    "\n",
    "# Adding Batch Normalization\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding='same'))\n",
    "\n",
    "model.add(Conv2D(filters=10, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=10, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Adding drop out\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1000 , activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "# see summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tqot2czl3Pbt",
    "outputId": "6e13b7d5-923b-4bff-a5ee-d98caabb5ade"
   },
   "outputs": [],
   "source": [
    "# compiling the model with adam optmizer and specifying the accuracy metrics (top-1 and top-5 accuracy)\n",
    "acc_metrics = [tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1, name='Top_1_Acc') , \n",
    "               tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5, name='Top_5_Acc') ]\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics= acc_metrics) \n",
    "\n",
    "\n",
    "# training the model \n",
    "num_epochs = 100\n",
    "mini_batch_size  = 128\n",
    "num_mini_batches = 85\n",
    "num_full_train_samples = data_train_p.shape[0]\n",
    "num_train_samples_per_epoch = mini_batch_size*num_mini_batches\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_top_1_acc = []\n",
    "train_top_5_acc = []\n",
    "test_top_1_acc = []\n",
    "test_top_5_acc = []\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    indices = np.random.choice(num_full_train_samples , size=(num_train_samples_per_epoch,) , replace=False)\n",
    "    data_train_set = data_train_p[indices]\n",
    "    label_train_set = label_train[indices]\n",
    "    H = model.fit(data_train_set, label_train_set, validation_data= (data_test_p, label_test) , batch_size= mini_batch_size, epochs=1)\n",
    "    train_losses.append(H.history[\"loss\"])\n",
    "    test_losses.append(H.history[\"val_loss\"])\n",
    "    train_top_1_acc.append(H.history[\"Top_1_Acc\"])\n",
    "    train_top_5_acc.append(H.history[\"Top_5_Acc\"])\n",
    "    test_top_1_acc.append(H.history[\"val_Top_1_Acc\"])\n",
    "    test_top_5_acc.append(H.history[\"val_Top_5_Acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 574
    },
    "id": "_l-nEJ2lFo0O",
    "outputId": "a7fed1c0-8dbd-4e2b-a73d-79a514e50fa4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plotting the results \n",
    "train_top_1_error = (1 - np.array(train_top_1_acc))*100\n",
    "test_top_1_error  = (1 - np.array(test_top_1_acc))*100\n",
    "train_top_5_error = (1 - np.array(train_top_5_acc))*100\n",
    "test_top_5_error  = (1 - np.array(test_top_5_acc))*100\n",
    "\n",
    "epochs = np.arange(0,num_epochs,1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_losses, label=\"train loss\")\n",
    "plt.plot(epochs, test_losses, label=\"test loss\" )\n",
    "plt.title(\"Training and Test loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_top_1_error, label=\"top-1 train error rate\")\n",
    "plt.plot(epochs, test_top_1_error, label=\"top-1 test error rate\")\n",
    "plt.plot(epochs, train_top_5_error, label=\"top-5 train error rate\")\n",
    "plt.plot(epochs, test_top_5_error, label=\"top-5 test error rate\")\n",
    "plt.title(\"Top-1 and top-5 train and test set error rates\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As You can see the testing loss for our model peaks at around 30 epochs and at around 80 epochs it stays flat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2heLLX0d3Pbu"
   },
   "source": [
    "### Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_26jtY3j3Pbu",
    "outputId": "a36106d3-e1a2-4c40-db0f-fb660f8479fd"
   },
   "outputs": [],
   "source": [
    "# defining the model \n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), strides=(1,1), padding=\"same\", activation=\"relu\", input_shape=(32,32,3), kernel_regularizer='l2'))\n",
    "# Adding Batch Normalization\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), strides=(1,1), padding=\"same\", activation=\"relu\", input_shape=(32,32,3), kernel_regularizer='l2'))\n",
    "# Adding Batch Normalization\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding='same'))\n",
    "\n",
    "# Adding drop out\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Adding Batch Normalization\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding='same'))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Adding drop out\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512 , activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "# see summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_13vpd8gi5uL"
   },
   "source": [
    "### In our final model we make modification to model 4 in order to produce a more accurcate model. Our first step is by adding l2 regularization by adding \"kernel_regularizer='l2'\" tp the dense layer. We also added more drop out layers to make sure we don't overfit given the increase in paramters. Lastly, we modified the flatten from 1,000 to 512 in order to minimize the amount of paramters used. Our modifications significantly imrpoved from Model 4. Our newest model val_Top_1_Acc: 0.7826"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JtDicV2-3Pbu",
    "outputId": "61b461eb-c7ae-4344-f4e4-ef346be44d7d"
   },
   "outputs": [],
   "source": [
    "# compiling the model with adam optmizer and specifying the accuracy metrics (top-1 and top-5 accuracy)\n",
    "acc_metrics = [tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1, name='Top_1_Acc') , \n",
    "               tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5, name='Top_5_Acc') ]\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics= acc_metrics) \n",
    "\n",
    "\n",
    "# training the model \n",
    "num_epochs = 100\n",
    "mini_batch_size  = 128\n",
    "num_mini_batches = 100\n",
    "num_full_train_samples = data_train_p.shape[0]\n",
    "num_train_samples_per_epoch = mini_batch_size*num_mini_batches\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_top_1_acc = []\n",
    "train_top_5_acc = []\n",
    "test_top_1_acc = []\n",
    "test_top_5_acc = []\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    indices = np.random.choice(num_full_train_samples , size=(num_train_samples_per_epoch,) , replace = False)\n",
    "    data_train_set = data_train_p[indices]\n",
    "    label_train_set = label_train[indices]\n",
    "    H = model.fit(data_train_set, label_train_set, validation_data= (data_test_p, label_test) , batch_size= mini_batch_size, epochs=1)\n",
    "    train_losses.append(H.history[\"loss\"])\n",
    "    test_losses.append(H.history[\"val_loss\"])\n",
    "    train_top_1_acc.append(H.history[\"Top_1_Acc\"])\n",
    "    train_top_5_acc.append(H.history[\"Top_5_Acc\"])\n",
    "    test_top_1_acc.append(H.history[\"val_Top_1_Acc\"])\n",
    "    test_top_5_acc.append(H.history[\"val_Top_5_Acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 574
    },
    "id": "LxksB2SvFm2z",
    "outputId": "aabe69de-6345-4302-e158-e9cc06a94a62"
   },
   "outputs": [],
   "source": [
    "# plotting the results \n",
    "train_top_1_error = (1 - np.array(train_top_1_acc))*100\n",
    "test_top_1_error  = (1 - np.array(test_top_1_acc))*100\n",
    "train_top_5_error = (1 - np.array(train_top_5_acc))*100\n",
    "test_top_5_error  = (1 - np.array(test_top_5_acc))*100\n",
    "\n",
    "epochs = np.arange(0,num_epochs,1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_losses, label=\"train loss\")\n",
    "plt.plot(epochs, test_losses, label=\"test loss\" )\n",
    "plt.title(\"Training and Test loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_top_1_error, label=\"top-1 train error rate\")\n",
    "plt.plot(epochs, test_top_1_error, label=\"top-1 test error rate\")\n",
    "plt.plot(epochs, train_top_5_error, label=\"top-5 train error rate\")\n",
    "plt.plot(epochs, test_top_5_error, label=\"top-5 test error rate\")\n",
    "plt.title(\"Top-1 and top-5 train and test set error rates\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "id": "G1ZOSFU-CoSZ",
    "outputId": "ca2e02a9-b30a-48ac-f9dd-1b9c1c02056d"
   },
   "outputs": [],
   "source": [
    "# list with cifar10 label names\n",
    "cifar10_label_names = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "\n",
    "# loading image and true label\n",
    "index = 4\n",
    "input_img = data_test_p[index]\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(input_img)\n",
    "plt.title(\"Sample image with label {}:{}\".format(label_test[index], cifar10_label_names[label_test[index][0]]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of a prediction using our best model (Model 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iyJDkv8kqqGF",
    "outputId": "9321562e-d314-4db8-bc8a-bedbae6d47b7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prediction using trained model\n",
    "y_pred = model.predict(np.array([input_img]))\n",
    "print(\"Class prediction probabilities : \\n {}\".format(y_pred))\n",
    "print(\"Model prediction- {}:{}\".format(np.argmax(y_pred), cifar10_label_names[np.argmax(y_pred)]))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "AI_Project_v4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
